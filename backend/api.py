"""
Arxis SOC Backend API

Main FastAPI application serving:
1. Log ingestion endpoint
2. Frontend data endpoints
3. System metrics

This is the orchestration layer connecting all components.
"""

import uuid
import asyncio
from datetime import datetime, timezone
from typing import List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from models import SecurityLog, Alert, MetricsSummary, Severity
from storage import storage
from detection_engine import detection_engine
from agents.crew_system import run_agent_analysis


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Background Processing
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

async def process_pending_signals():
    """Background task: Process signals with CrewAI agents."""
    while True:
        try:
            pending = storage.get_pending_signals()
            
            for signal_dict in pending:
                # Convert to DetectionSignal object
                from models import DetectionSignal
                signal = DetectionSignal(**signal_dict)
                
                print(f"\nğŸ¤– Processing signal {signal.signal_id} with AI agents...")
                
                # Run agent analysis (this may take 10-30 seconds)
                agent_output = run_agent_analysis(signal)
                
                # Parse agent output into alert
                alert = Alert(
                    alert_id=str(uuid.uuid4()),
                    timestamp=datetime.now(timezone.utc).isoformat(),
                    user=signal.user,
                    threat_type=signal.signal_type.value,
                    severity=signal.severity,
                    explanation=agent_output.get("result", "No explanation available"),
                    recommendation="Review immediately and verify user identity",
                    agent_trace=agent_output.get("agent_trace", []),
                    raw_events=[event.model_dump() if hasattr(event, 'model_dump') else event 
                               for event in signal.events],
                    metadata={
                        "signal_id": signal.signal_id,
                        "agent_success": agent_output.get("success", False),
                        **signal.metadata
                    }
                )
                
                # Store alert
                storage.add_alert(alert)
                storage.mark_signal_processed(signal.signal_id)
                
                print(f"âœ… Alert {alert.alert_id} created from signal {signal.signal_id}")
        
        except Exception as e:
            print(f"âš ï¸  Error in background processing: {e}")
        
        # Check every 5 seconds
        await asyncio.sleep(5)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup/shutdown lifecycle."""
    # Start background task for processing signals
    task = asyncio.create_task(process_pending_signals())
    print("âœ… Background signal processor started")
    
    yield
    
    # Shutdown
    task.cancel()
    print("ğŸ›‘ Background processor stopped")


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# FastAPI Application
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

app = FastAPI(
    title="Arxis SOC Backend",
    description="AI-native Security Operations Center POC",
    version="0.2.0",
    lifespan=lifespan
)

# CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify frontend domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Ingestion Endpoint
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.post("/logs", status_code=200)
async def ingest_log(log: SecurityLog):
    """
    Ingest a security log.
    
    Flow:
    1. Store log
    2. Run detection engine
    3. If signal detected â†’ queue for agent processing
    """
    # Store log
    storage.add_log(log)
    
    # Run detection
    signal = detection_engine.analyze(log)
    
    if signal:
        # Store signal for background processing
        storage.add_signal(signal)
        print(f"ğŸš¨ Detection: {signal.signal_type.value} for {signal.user}")
        
        return {
            "status": "detected",
            "signal_type": signal.signal_type.value,
            "signal_id": signal.signal_id
        }
    
    return {"status": "ok"}


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Frontend API Endpoints
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.get("/alerts", response_model=List[Alert])
async def get_alerts(
    severity: str = None,
    limit: int = 100
):
    """
    Get all alerts, optionally filtered by severity.
    
    Query params:
    - severity: Filter by severity (LOW/MEDIUM/HIGH/CRITICAL)
    - limit: Max number of alerts to return
    """
    if severity:
        try:
            sev = Severity(severity.upper())
            alerts = storage.get_alerts_by_severity(sev)
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid severity level")
    else:
        alerts = storage.get_all_alerts()
    
    # Convert to Alert objects and apply limit
    alert_objects = [Alert(**alert) for alert in alerts[-limit:]]
    
    return alert_objects


@app.get("/alerts/{alert_id}", response_model=Alert)
async def get_alert(alert_id: str):
    """
    Get a specific alert by ID.
    """
    alert = storage.get_alert_by_id(alert_id)
    
    if not alert:
        raise HTTPException(status_code=404, detail="Alert not found")
    
    return Alert(**alert)


@app.get("/metrics", response_model=MetricsSummary)
async def get_metrics():
    """
    Get system metrics for dashboard.
    
    Returns:
    - Total logs processed
    - Total alerts generated
    - Alerts by severity
    - Recent activity
    """
    metrics = storage.get_metrics()
    return MetricsSummary(**metrics)


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "service": "arxis-soc-backend",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }


@app.get("/metrics/realtime")
async def get_realtime_metrics():
    """
    Get real-time system metrics for heartbeat visualization.
    
    Returns activity data for:
    - Log Collector: based on recent log ingest rate
    - Threat Intelligence: based on detection signals
    - SIEM Engine: based on overall processing
    - Alert Pipeline: based on alert creation rate
    - Analytics Engine: based on agent processing
    - Database: based on storage operations
    """
    import random
    
    # Get recent activity from storage
    recent_logs = storage.get_recent_logs(100)
    recent_alerts = storage.get_all_alerts()[-20:]
    pending_signals = storage.get_pending_signals()
    
    # Calculate activity rates (events in last minute simulation)
    log_rate = len(recent_logs) if recent_logs else 0
    alert_rate = len(recent_alerts) if recent_alerts else 0
    signal_rate = len(pending_signals) if pending_signals else 0
    
    # Generate realistic latency values based on actual activity
    # More activity = slightly higher latency
    def compute_latency(base: int, activity: int, variance: int = 10) -> int:
        load_factor = min(activity / 20, 1.5)  # Cap at 1.5x
        jitter = random.randint(-variance, variance)
        return max(5, int(base * load_factor + jitter))
    
    # Generate history arrays (last 10 readings) with realistic variance
    def generate_history(base: int, activity: int) -> list:
        history = []
        for _ in range(10):
            history.append(compute_latency(base, activity, 5))
        return history
    
    components = [
        {
            "id": "1",
            "name": "Log Collector",
            "status": "healthy" if log_rate > 0 else "degraded",
            "latency": compute_latency(45, log_rate),
            "history": generate_history(45, log_rate),
            "activity": log_rate
        },
        {
            "id": "2", 
            "name": "Threat Intelligence",
            "status": "healthy",
            "latency": compute_latency(120, signal_rate),
            "history": generate_history(120, signal_rate),
            "activity": signal_rate
        },
        {
            "id": "3",
            "name": "SIEM Engine",
            "status": "healthy",
            "latency": compute_latency(85, log_rate + signal_rate),
            "history": generate_history(85, log_rate + signal_rate),
            "activity": log_rate + signal_rate
        },
        {
            "id": "4",
            "name": "Alert Pipeline",
            "status": "healthy" if signal_rate == 0 else "degraded",
            "latency": compute_latency(150, alert_rate + signal_rate * 3),
            "history": generate_history(150, alert_rate + signal_rate * 3),
            "activity": alert_rate
        },
        {
            "id": "5",
            "name": "Analytics Engine",
            "status": "healthy",
            "latency": compute_latency(310, alert_rate),
            "history": generate_history(310, alert_rate),
            "activity": alert_rate
        },
        {
            "id": "6",
            "name": "Database",
            "status": "healthy",
            "latency": compute_latency(12, log_rate + alert_rate),
            "history": generate_history(12, log_rate + alert_rate),
            "activity": log_rate + alert_rate
        }
    ]
    
    return {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "components": components,
        "summary": {
            "total_logs": log_rate,
            "total_alerts": alert_rate,
            "pending_signals": signal_rate
        }
    }


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Debug Endpoints (POC only)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@app.get("/debug/logs")
async def debug_logs(limit: int = 50):
    """Get recent logs for debugging."""
    return storage.get_recent_logs(limit)


@app.get("/debug/signals")
async def debug_signals():
    """Get all signals for debugging."""
    return {
        "pending": storage.get_pending_signals(),
        "total": len(storage.signals)
    }


@app.post("/debug/clear")
async def debug_clear():
    """Clear all data (for testing)."""
    storage.clear_all()
    return {"status": "cleared"}


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Entrypoint
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    import uvicorn
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                           â•‘
    â•‘           ğŸ›¡ï¸  ARXIS SOC BACKEND ğŸ›¡ï¸         â•‘
    â•‘                                           â•‘
    â•‘  AI-Native Security Operations Center     â•‘
    â•‘  Powered by CrewAI                        â•‘
    â•‘                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
